{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: PySpark - II\n",
    "### CS186, UC Berkeley, Spring 2016\n",
    "### Due: Thursday Feb 25, 2016, 11:59 PM\n",
    "### Note: **This homework is to be done individually!  Do not modify any existing method signatures.**\n",
    "### **This is the second of two .ipynb files in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On some computers it may be possible to run this lab \n",
    "## locally by using this script; you will need to run\n",
    "## this each time you start the notebook.\n",
    "## You do not need to run this on inst machines.\n",
    "\n",
    "from local_install import setup_environment\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from utils import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CleanRDD\n",
    "from utils import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CacheMap\n",
    "\n",
    "In this part, we'll construct an rdd that is backed by a `ClockMap` and will behave like `rdd.map(func)`.  \n",
    "First, implement the `ClockMap` class so that it maintains a cache (of limited `cacheSize`) using the clock replacement policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClockMap:\n",
    "    \n",
    "    def __init__(self, cacheSize, func):\n",
    "        \"\"\"\n",
    "        Do not change existing variables.\n",
    "        [Optional] You are free to add additional items and methods.\n",
    "        \"\"\"\n",
    "        self.cacheSize = cacheSize\n",
    "        self.fn = func\n",
    "        self._p = 0 # pointer\n",
    "        self._increments = 0\n",
    "        self._miss_count = 0\n",
    "        self.buffers = [[None, 0] for x in range(cacheSize)]\n",
    "        self.items_to_index = {}\n",
    "        \n",
    "    def _increment(self):\n",
    "        \"\"\"\n",
    "        Do not change this method.\n",
    "        Updates the clock pointer. The modulo maintains the clock nature.\n",
    "        \"\"\"\n",
    "        self._increments += 1\n",
    "        self._p = (self._p + 1) % self.cacheSize\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"\n",
    "        Returns func(k) using the buffer to cache limited results.\n",
    "        \n",
    "        :param k: Value to be evaluated\n",
    "        \n",
    "        >>> clock = ClockMap(4, lambda x: x ** 2)\n",
    "        >>> clock[4]\n",
    "        16\n",
    "        >>> clock[3]\n",
    "        9\n",
    "        >>> clock._p\n",
    "        2\n",
    "        \"\"\"\n",
    "        if k in self.items_to_index:\n",
    "            i = self.items_to_index[k]\n",
    "            buf = self.buffers[i]\n",
    "            buf[1] = 1\n",
    "            return buf[0][1]\n",
    "        else:\n",
    "            buf = self.buffers[self._p]\n",
    "            while buf[1] > 0:\n",
    "                buf[1] = 0\n",
    "                self._increment()\n",
    "                buf = self.buffers[self._p]\n",
    "            \n",
    "            if buf[0] is not None:\n",
    "                pre_k = buf[0][0]\n",
    "                del self.items_to_index[pre_k]\n",
    "            buf[0] = (k, self.fn(k))\n",
    "            buf[1] = 1\n",
    "            self.items_to_index[k] = self._p\n",
    "            self._increment()\n",
    "            return buf[0][1]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `cacheMap`, which will return an rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cacheMap(rdd, cacheSize, func):\n",
    "    \"\"\"\n",
    "    Returns an RDD that behaves like rdd.map(func) but\n",
    "    is implemented using the ClockMap.\n",
    "    \n",
    "    :param rdd: Given RDD\n",
    "    :param cacheSize: Number of cache/buffer pages in the ClockMap\n",
    "    :param func: Function to map with\n",
    "    \"\"\"\n",
    "    clock_map = ClockMap(cacheSize, func)\n",
    "    return rdd.mapPartitionsWithIndex(lambda _, itr: (clock_map[i] for i in itr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free test for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "clock = ClockMap(4, lambda x: x ** 2)\n",
    "print clock[4], clock[3]\n",
    "print clock._p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "16, 9\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: External Algorithms\n",
    "\n",
    "You'll need an understanding of the partitioning step of external hashing, and the divide step of external sorting (recall the lecture on external algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import itertools\n",
    "import bisect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some tools you may want to use (examples use cases included). You should Google the unfamiliar ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itertools.islice\n",
    "generator = (y for y in range(100))\n",
    "test1 = itertools.islice(generator, 5)\n",
    "next(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heapq.merge\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "generator2 = (even for even in range(100)[::2])\n",
    "key = lambda x: x\n",
    "test2 = heapq.merge([generator1, generator2], key=key, reverse=False)\n",
    "next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we insert 3, it goes to 1\n",
      "If we insert 1, it goes to 0\n",
      "If we insert 4, it goes to 1\n"
     ]
    }
   ],
   "source": [
    "# bisect.bisect_left\n",
    "buckets = [2, 4, 4]\n",
    "print \"If we insert 3, it goes to %d\" % bisect.bisect_left(buckets, 3)\n",
    "print \"If we insert 1, it goes to %d\" % bisect.bisect_left(buckets, 1)\n",
    "print \"If we insert 4, it goes to %d\" % bisect.bisect_left(buckets, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 35, 45, 65, 67, 68, 69, 84, 85]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.sample\n",
    "rdd = sc.parallelize(range(100))\n",
    "fraction = 0.1\n",
    "rdd.sample(False, fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Serializer and os.unlink (Serializer is provided via utils.GeneralTools)\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "filename = \"temp\"\n",
    "with open(filename, \"w\") as f:\n",
    "    serializer.dump_stream(generator1, f)\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    stream = serializer.load_stream(f)\n",
    "    print next(stream)\n",
    "\n",
    "os.unlink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_used_memory - returns an int in MB\n",
    "get_used_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to modify the following function - it should come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sort_dir(partId, n):\n",
    "    \"\"\"\n",
    "    Returns a path for temporary file.\n",
    "\n",
    "    :param n: Unique identification for file\n",
    "    \"\"\"\n",
    "    d = \"tmp/sort/\" + str(partId) + \"/\"\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    return os.path.join(d, str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def externalSortStream(iterator, partId=0, reverse=False, keyfunc=None, serial=serializer, limit=10, batch=100):\n",
    "    \"\"\"\n",
    "    Given an iterator, returns an iterator of sorted elements (according to parameters). \n",
    "    \n",
    "    :param iterator: iterator. Expects (Key, Value).\n",
    "    :param keyfunc: function applied on the keykey.\n",
    "    :param reverse: Reverse default ordering if true. (default is ascending; reverse is descending) \n",
    "    :param serializer: See README.\n",
    "    :param limit: memory limit.\n",
    "    :param batch: Number of elements to read at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_runs = [] # can be used to hold a list of iterators\n",
    "    run = [] # used to hold the current run of elements\n",
    "    \n",
    "    def load(fileobj):\n",
    "        \"\"\"\n",
    "        Returns a generator object that outputs elements \n",
    "        from a serialized (saved) stream. Closes the file when done.\n",
    "        \n",
    "        :param fileobj: python object file\n",
    "        \"\"\"\n",
    "        for _ in serial.load_stream(fileobj):\n",
    "            yield _\n",
    "        fileobj.close()\n",
    "   \n",
    "    # TODO everywhere below \n",
    "    \n",
    "    while True:\n",
    "        while True:\n",
    "            c = list(itertools.islice(iterator, batch))\n",
    "            run += c\n",
    "\n",
    "            # Base case, no more data need read\n",
    "            if get_used_memory() > limit or len(run) < batch:\n",
    "                break\n",
    "\n",
    "        if len(run) == 0:\n",
    "            break\n",
    "                \n",
    "        sorted_run = sorted(run, key=lambda x: keyfunc(x[0]), reverse=reverse)\n",
    "        run_file_name = get_sort_dir(partId, len(all_runs))\n",
    "        with open(run_file_name, \"w\") as f:\n",
    "            serializer.dump_stream(sorted_run, f)\n",
    "        f = open(run_file_name, \"r\")\n",
    "        all_runs.append(load(f))\n",
    "        os.unlink(run_file_name)\n",
    "        run = [] # clear cache\n",
    "    \n",
    "    return heapq.merge(all_runs, key=lambda x: keyfunc(x[0]), reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember to run the import box above.\n",
    "\n",
    "def partitionByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    Uses sampling to partitions the elements by the return value of \n",
    "    keyfunc.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "\n",
    "    if numPartitions is None:\n",
    "        numPartitions = rdd.getNumPartitions()\n",
    "\n",
    "    if numPartitions == 1:\n",
    "        if rdd.getNumPartitions() > 1:\n",
    "            rdd = rdd.coalesce(1)\n",
    "        return rdd\n",
    "    \n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    boundaries = getBuckets(rdd, ascending, numPartitions, keyfunc)\n",
    "    balanceLoad = lambda x: bisect.bisect_left(boundaries, x)\n",
    "    return rdd.partitionBy(numPartitions, balanceLoad)\n",
    "\n",
    "\n",
    "def getBuckets(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    [Optional] Returns a list of bucket boundaries of length (numPartitions - 1),\n",
    "    in an order as specfied by the given parameters: ascending, keyfunc. \n",
    "    Bucket boundaries are determined by sampling as specified in the README.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "    fraction = (10 * numPartitions) / rdd.count()\n",
    "    samples = sorted(rdd.sample(None, fraction).collect(), key=keyfunc, reverse=not ascending)\n",
    "    \n",
    "    return [x for x in samples[10::10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"\n",
    "    Returns an RDD after executing an external sort using \n",
    "    functions partitionByKey and externalSortStream. \n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyFunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    rdd = partitionByKey(rdd, ascending, numPartitions, keyfunc)\n",
    "    f = lambda i, itr: externalSortStream(itr, i, not ascending, keyfunc)\n",
    "    return rdd.mapPartitionsWithIndex(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tests for `partitionByKey` and `externalSortStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 7),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (5, 5),\n",
       " (9, 9),\n",
       " (4, 4),\n",
       " (3, 3),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stream = ((i, i) for i in range(100))\n",
    "list(externalSortStream(test_stream, keyfunc=(lambda x: abs(50 - (x ** 2)))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(7, 7),\n",
    " (6, 6),\n",
    " (8, 8),\n",
    " (5, 5),\n",
    " (9, 9),\n",
    " (4, 4),\n",
    " (3, 3),\n",
    " (2, 2),\n",
    " (1, 1),\n",
    " (0, 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.CleanRDD.CleanRDD at 0x7fda102d94d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(20), 4).map(lambda x: (x * 37 % 6, x ** 3 % 34)))\n",
    "partitionByKey(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look rather well-distributed. Try forcing a skewed distribution and observe how effective the partitioning is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test for `sortByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-81, 81),\n",
       " (-83, 83),\n",
       " (-85, 85),\n",
       " (-87, 87),\n",
       " (-89, 89),\n",
       " (-91, 91),\n",
       " (-93, 93),\n",
       " (-95, 95),\n",
       " (-97, 97),\n",
       " (-99, 99)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(100), 4).map(lambda x: (x *((-1) ** x) , x)))\n",
    "sortByKey(rdd, keyfunc=lambda key: key, ascending=False).collect()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(-81, 81),\n",
    " (-83, 83),\n",
    " (-85, 85),\n",
    " (-87, 87),\n",
    " (-89, 89),\n",
    " (-91, 91),\n",
    " (-93, 93),\n",
    " (-95, 95),\n",
    " (-97, 97),\n",
    " (-99, 99)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: PASS - task3ClockMap.txt matched reference output.\n",
      "Task 3: PASS - task3CacheMap.txt matched reference output.\n",
      "Task 4: PASS - task4.txt matched reference output.\n"
     ]
    }
   ],
   "source": [
    "tests.test3ClockMap(ClockMap)\n",
    "tests.test3CacheMap(cacheMap)\n",
    "tests.test4(sortByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
